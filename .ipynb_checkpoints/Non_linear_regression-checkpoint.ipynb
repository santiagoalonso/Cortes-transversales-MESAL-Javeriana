{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresiones no lineales\n",
    "\n",
    "Muchas relaciones en la realidad son no lineales. Por ejemplo, la utilidad de consumir varías hamburguesas. La primera puede ser muy útil si tengo hambre, depronto la segunda también, pero llega un punto de saturación. La utilidad no crece linealmente de forma irrestricta. En este modulo, aprenderemos técnicas para lidiar con situaciones similares donde la respuesta es no lineal en los predictores. En particular, será una introducción para correr modelos lineales generales en Python (GLM, por sus siglas en inglés)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "# para estructura de datos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# para gráficas e interacciones\n",
    "import matplotlib \n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sn\n",
    "import plotly \n",
    "import plotnine as p9\n",
    "from plotnine import ggplot, geom_point, geom_line, aes, geom_smooth, facet_wrap, themes\n",
    "import bqplot as bq\n",
    "from bqplot import pyplot as bqplt\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import GridspecLayout\n",
    "from bokeh.plotting import figure, output_notebook, output_file, show\n",
    "from bokeh.models import ColumnDataSource, Slider, Column, LinearInterpolator, CategoricalColorMapper, Legend, LegendItem\n",
    "from bokeh.layouts import row, column\n",
    "from bokeh.io import push_notebook\n",
    "from bokeh.palettes import all_palettes, gray, inferno, magma, viridis, cividis, turbo\n",
    "from bokeh.application import Application\n",
    "from bokeh.application.handlers import FunctionHandler\n",
    "\n",
    "\n",
    "# para estadística \n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm \n",
    "#from statsmodels.sandbox.regression.gmm import IV2SLS \n",
    "from linearmodels.iv import IV2SLS\n",
    "import scipy\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import gaussian_kde\n",
    "from pscore_match.match import Match, whichMatched\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "# Cancer (https://www.kaggle.com/uciml/breast-cancer-wisconsin-data/)\n",
    "# Tumor information (e.g. size, radius, other measures)\n",
    "cancer = pd.read_csv('DataSets/Cancer.csv')\n",
    "cancer.insert(0, 'diagnosis_recode', 1) #0 malignant, 1 benign\n",
    "cancer.loc[cancer['diagnosis'] == 'M','diagnosis_recode'] = 0\n",
    "\n",
    "\n",
    "#Police arrests (Data Analysis Using Regression and Multilevel/Hierarchical Models de Andrew Gelman & Jennifer Hill)\n",
    "Police = pd.read_csv(\"DataSets/GelmanHill_reg_hierarchical/frisk_with_noise.csv\")\n",
    "#precincts are numbered\n",
    "#ethnicity (eth) 1=black, 2=hispanic, 3=white\n",
    "#crime type 1=violent, 2=weapons, 3=property, 4=drug\n",
    "Police = Police.drop(Police[Police['past.arrests'] == 0].index) #only one row removed\n",
    "Police = Police.astype({'eth': 'category', 'precinct':'category'})\n",
    "\n",
    "#Water wells (Arsenic)\n",
    "wells = pd.read_stata('DataSets/GelmanHill_reg_hierarchical//arsenic/all.dta')\n",
    "wells_itr = pd.read_stata('DataSets/GelmanHill_reg_hierarchical//arsenic/all.dta', iterator=True) #tiene, entre otras cosas, la descripción de las variables\n",
    "wells_itr.variable_labels() #descripción de columnas\n",
    "\n",
    "\n",
    "#Storable votes\n",
    "storable = pd.read_csv(\"DataSets/GelmanHill_reg_hierarchical/storablevotes/6playergames.csv\")\n",
    "#the column value was randomly (uniform) assigned to subjects at the start of a round and before casting a vote for theme 1 \n",
    "\n",
    "\n",
    "#Fishing mode\n",
    "fishing = pd.read_stata(\"DataSets/mus15data.dta\")\n",
    "fishing_itr = pd.read_stata('DataSets/mus15data.dta', iterator=True) #tiene, entre otras cosas, la descripción de las variables\n",
    "fishing_itr.variable_labels() #descripción de columnas\n",
    "\n",
    "\n",
    "\n",
    "#HISP (Health Insurance Subsidy Program)\n",
    "HISP = pd.read_stata('DataSets/HISP/evaluation.dta')\n",
    "HISP_itr = pd.read_stata('DataSets/HISP/evaluation.dta', iterator=True) #tiene, entre otras cosas, la descripción de las variables\n",
    "HISP_itr.variable_labels()\n",
    "\n",
    "\n",
    "#M & M\n",
    "#Fuente: https://joshmadison.com/2007/12/02/mms-color-distribution-analysis/\n",
    "mandm = pd.read_csv(\"DataSets/mandm.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualización: distribuciones y regresiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una regresión OLS se puede escribir de la siguiente forma:\n",
    "\n",
    "$$ Y \\sim  Normal(link(\\beta_0 + \\beta_1X), \\sigma^2)  $$\n",
    "\n",
    "Link puede ser cualquier función. En el caso de una regresión tradicional, link es la identidad.\n",
    "\n",
    "$$ Y \\sim  Normal(\\beta_0 + \\beta_1X, \\sigma^2)  $$\n",
    "\n",
    "¿Podemos usar otras distribuciones diferentes a la normal? ¿Otras funciones link diferentes a identidad? Si, con generalized linear models. Antes de ver cómo se hace en Python, veamos esta idea de que las regresiones son distribuciones. \n",
    "\n",
    "Vamos a ver como sube o baja el promedio de la distribución normal, `Y`, a medida que cambia el valor de las variables independientes, `X`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# prepare some data\n",
    "nsamples = 500\n",
    "noise = 500\n",
    "betas = [4, 10.5, 0.5]\n",
    "d = {'x1' : np.random.normal(10, 40, size = nsamples),\n",
    "    'x2' : np.random.normal(1, 4, size = nsamples)}\n",
    "d['y'] = betas[0] + betas[1]*d['x1'] + betas[2]*d['x2']\n",
    "d['y_noise'] = d['y'] + np.random.normal(0, noise, size = nsamples)\n",
    "dataT = pd.DataFrame(d)\n",
    "\n",
    "model = smf.ols('y_noise ~ x1 + x2 ', data = dataT)\n",
    "results = model.fit()\n",
    "predictions = results.predict(dataT[['x1','x2']])\n",
    "dataT['predictions'] = predictions\n",
    "#print(results.summary())\n",
    "source = ColumnDataSource(dataT) #Bokeh needs data in this format\n",
    "\n",
    "\n",
    "# output to notebook inline or html file\n",
    "output_notebook()\n",
    "#output_file(\"lines.html\") #creates a html file (can be shared)\n",
    "\n",
    "## create scales for desired attributes\n",
    "#color_scale = CategoricalColorMapper(factors=list(df.species.unique()), palette=all_palettes['Colorblind'])\n",
    "size_scale = LinearInterpolator(x=[min(dataT['x2']), max(dataT['x2'])], y=[2, 8]) #y is the range of size of dots\n",
    "\n",
    "\n",
    "# create new plots with a title and axis labels\n",
    "p = figure(title=\"Regresión\", x_axis_label='x1', y_axis_label='y',\n",
    "           tools=\"crosshair,pan,reset,save,wheel_zoom\",\n",
    "           x_range=[-150, 150], y_range=[-2200, 2200],\n",
    "           plot_height=300, plot_width=500)\n",
    "\n",
    "# add renderers\n",
    "dot1 = p.circle('x1', 'y_noise', \n",
    "      size = dict(field='x2', transform=size_scale), color = '#C0C0C0',\n",
    "      source = source)\n",
    "dot0 = p.circle('x1', 'predictions', \n",
    "             size = dict(field='x2', transform=size_scale), color = 'forestgreen',\n",
    "             source = source)\n",
    "plot_data  = ColumnDataSource({'x1': [0], 'x2': [0], 'y' : [100]}) #init plot.data\n",
    "dot2 = p.circle('x1', 'y',\n",
    "      size = 10, color = '#FF4500', alpha = 0, \n",
    "      source = plot_data) \n",
    "\n",
    "legend = Legend(items=[LegendItem(label=\"predicción (media)\", renderers=[dot0]),\n",
    "                       LegendItem(label=\"predicción (muestras)\", renderers=[dot2])],\n",
    "               location='top_left',\n",
    "               label_text_font_size = '7pt')\n",
    "p.add_layout(legend)\n",
    "\n",
    "\n",
    "LINE_ARGS = dict(color=\"#3A5785\", line_color=None)\n",
    "vhist, vedges = np.histogram(dataT['y_noise'], density=True, bins=20) #vertical histogram\n",
    "vzeros = np.zeros(len(vedges)-1)\n",
    "vmax = max(vhist)*1.1\n",
    "pv = figure(title = \"Muestras Y\", toolbar_location=None, plot_width=100, plot_height = p.plot_height, \n",
    "            x_range=(0, 1.7*vmax), y_range=p.y_range, min_border=10, y_axis_location=\"right\")\n",
    "pv.ygrid.grid_line_color = None\n",
    "pv.xaxis.major_label_orientation = np.pi/4\n",
    "pv.background_fill_color = \"#ffffff\"\n",
    "pv.xgrid.visible = False\n",
    "pv.ygrid.visible = False\n",
    "pv.xaxis.visible = False\n",
    "pv.yaxis.visible = False\n",
    "\n",
    "source_hist = ColumnDataSource({\"left\": np.repeat(0,len(vedges)-1), \n",
    "                                \"bottom\": vedges[:-1], \"top\": vedges[1:], \"right\": vhist})\n",
    "vh = pv.quad(left='left', bottom='bottom', top='top', right='right', source = source_hist,\n",
    "             color=\"white\", line_color=\"#000000\")\n",
    "x = dataT['y_noise'].sort_values()\n",
    "pdf = scipy.stats.norm.pdf\n",
    "source_density = ColumnDataSource({'x': pdf(x, betas[0] + betas[1]*dataT['x1'].mean() + betas[2]*dataT['x2'].mean(), noise),\n",
    "                                  'y': x})\n",
    "vd = pv.line(x='x', y = 'y',\n",
    "        source = source_density, line_color = '#FF4500', line_width = 2)\n",
    "\n",
    "\n",
    "#Set widgets\n",
    "sX1 = Slider(start = d['x1'].min(), end = d['x1'].max(), value=0, step=1, title=\"x1\", width = 175)\n",
    "sX2 = Slider(start = d['x2'].min(), end = d['x2'].max(), value=0, step=.5, title=\"x2\", width = 175)\n",
    "\n",
    "\n",
    "# Define interactivity functions\n",
    "from time import sleep\n",
    "def my_widget_input_handler(attr, old, new):\n",
    "    x1 = np.repeat(sX1.value, nsamples)\n",
    "    x2 = np.repeat(sX2.value, nsamples)\n",
    "    y = betas[0] + betas[1]*x1 + betas[2]*x2  + np.random.normal(0, 0.75*noise, size = nsamples)\n",
    "    \n",
    "    plot_data.data = {'x1': x1, 'x2': x2, 'y': y}\n",
    "    #dot2.glyph.fill_color = '#FF4500'\n",
    "    dot2.glyph.fill_alpha = 1\n",
    "    dot2.glyph.line_alpha = 1\n",
    "    dot2.glyph.size = 0.5*abs(sX2.value) + 1\n",
    "    \n",
    "    vhist, vedges = np.histogram(y, density=True, bins=20) #vertical histogram\n",
    "    vzeros = np.zeros(len(vedges)-1)\n",
    "    vmax = max(vhist)*1.1\n",
    "    source_hist.data = {\"left\": np.repeat(0,len(vedges)-1), \"bottom\": vedges[:-1], \n",
    "                        \"top\": vedges[1:], \"right\": vhist}\n",
    "    \n",
    "    x = pd.Series(y).sort_values()\n",
    "    source_density.data = {'x': pdf(x, betas[0] + betas[1]*x1 + betas[2]*x2, 0.75*noise), \n",
    "                           'y': x}\n",
    "\n",
    "\n",
    "# Define widgets callbacks\n",
    "sX1.on_change('value', my_widget_input_handler)\n",
    "sX2.on_change('value', my_widget_input_handler)\n",
    "\n",
    "\n",
    "# Show the plot\n",
    "layout = row(p, pv, column(sX1,sX2))\n",
    "def modify_doc(doc):\n",
    "    doc.add_root(row(layout, width=50))\n",
    "    doc.title = \"Sliders\"\n",
    "    \n",
    "handler = FunctionHandler(modify_doc)\n",
    "app = Application(handler)\n",
    "show(app)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la anterior visualización, vemos como la formula de la regresión es el promedio de una distribución normal. \n",
    "\n",
    "$$ Y \\sim  Normal(\\beta_0 + \\beta_1X, \\sigma^2)  $$\n",
    "\n",
    "Las distribuciones normales pueden tomar valores positivos y negativos. Es apropiada, entonces, si la variable `Y` toma valores normales y lineales. Pero veamos un ejemplo que no es así: variables binarias (e.g. gano/perdio, vivo/muerto, 1/0, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# NOTA: hay que correr la anterior celda\n",
    "# prepare some data\n",
    "nsamples = 1000 \n",
    "def logistic(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "betas = [1, 2, 0.15]\n",
    "d = {'x1' : np.random.normal(0, 5, size = nsamples),\n",
    "     'x2' : np.random.normal(0, 5, size = nsamples)}\n",
    "d['y'] = logistic(betas[0] + betas[1]*d['x1'] + betas[2]*d['x2']) #y son probabilidades!\n",
    "d['y_noise'] = np.random.binomial(n = 1, p = d['y'])\n",
    "dataT = pd.DataFrame(d)\n",
    "\n",
    "model = smf.glm('y_noise ~ x1 + x2 ', data = dataT, \n",
    "                family = sm.families.Binomial())\n",
    "results = model.fit()\n",
    "predictions = results.predict(dataT[['x1','x2']])\n",
    "dataT['predictions'] = predictions #Note como las predicciones de Y estan en [0,1]\n",
    "#print(results.summary())\n",
    "source = dataT #Bokeh needs data in this format\n",
    "\n",
    "# output to notebook inline or html file\n",
    "output_notebook()\n",
    "\n",
    "## create scales for desired attributes\n",
    "#color_scale = CategoricalColorMapper(factors=list(df.species.unique()), palette=all_palettes['Colorblind'])\n",
    "size_scale = LinearInterpolator(x=[min(dataT['x2']), max(dataT['x2'])], y=[2, 10]) #y is the range of size of dots\n",
    "\n",
    "\n",
    "# create new plots with a title and axis labels\n",
    "p = figure(title=\"Regresión\", x_axis_label='x1', y_axis_label='y',\n",
    "           tools=\"crosshair,pan,reset,save,wheel_zoom\",\n",
    "           x_range=[-20, 20], y_range=[-0.1, 1.1],\n",
    "           plot_height=300, plot_width=500)\n",
    "\n",
    "# add renderers\n",
    "circle1 = p.circle('x1', 'y_noise', \n",
    "                   size = dict(field='x2', transform=size_scale), color = '#C0C0C0',\n",
    "                   source = source)\n",
    "circle2 = p.circle('x1', 'predictions', \n",
    "                   color = 'forestgreen', alpha =0.5,\n",
    "                   size = dict(field='x2', transform=size_scale),\n",
    "                   source = source)\n",
    "\n",
    "plot_data  = ColumnDataSource({'x1': [0], 'x2': [0], 'y' : [100]}) #init plot data\n",
    "circle3 = p.circle('x1', 'y',\n",
    "      size = 10, color = '#FF4500', alpha = 0, \n",
    "      source = plot_data) \n",
    "\n",
    "legend = Legend(items=[LegendItem(label=\"Data\", renderers=[circle1]),\n",
    "                       LegendItem(label=\"predicción (prob. que Y sea 1)\", renderers=[circle2]),\n",
    "                       LegendItem(label=\"predicción (muestras)\", renderers=[circle3])],\n",
    "               location='top_left',\n",
    "               label_text_font_size = '7pt')\n",
    "p.add_layout(legend)\n",
    "\n",
    "\n",
    "LINE_ARGS = dict(color=\"#3A5785\", line_color=None)\n",
    "vhist, vedges = np.histogram(dataT['y_noise'], density=True, bins=20) #vertical histogram\n",
    "vzeros = np.zeros(len(vedges)-1)\n",
    "vmax = max(vhist)*1.1\n",
    "pv = figure(title = \"Muestras Y\", toolbar_location=None, plot_width=100, plot_height = p.plot_height, \n",
    "            x_range=(0, 1.1), y_range=p.y_range, min_border=10, y_axis_location=\"right\")\n",
    "pv.ygrid.grid_line_color = None\n",
    "pv.xaxis.major_label_orientation = np.pi/4\n",
    "pv.background_fill_color = \"#ffffff\"\n",
    "pv.xgrid.visible = False\n",
    "pv.ygrid.visible = False\n",
    "pv.xaxis.visible = True\n",
    "pv.yaxis.visible = False\n",
    "\n",
    "source_hist = ColumnDataSource({\"left\": np.repeat(0,len(vedges)-1), \n",
    "                                \"bottom\": vedges[:-1], \"top\": vedges[1:], \"right\": vhist/vhist.sum()})\n",
    "vh = pv.quad(left='left', bottom='bottom', top='top', right='right', source = source_hist,\n",
    "             color=\"white\", line_color=\"#000000\")\n",
    "x = dataT['y_noise'].sort_values()\n",
    "pdf = scipy.stats.binom.pmf\n",
    "n = 1\n",
    "pr = logistic(betas[0] + betas[1]*dataT['x1'].mean() + betas[2]*dataT['x2'].mean())\n",
    "source_density = ColumnDataSource({'x': pdf(x, n, pr),\n",
    "                                   'y': x})\n",
    "vd = pv.line(x='x', y = 'y', alpha = 0,\n",
    "             source = source_density, line_color = '#FF4500', line_width = 2)\n",
    "\n",
    "\n",
    "#Set widgets\n",
    "sX1 = Slider(start = d['x1'].min(), end = d['x1'].max(), value=0, step=.01, title=\"x1\", width = 175)\n",
    "sX2 = Slider(start = d['x2'].min(), end = d['x2'].max(), value=0, step=.05, title=\"x2\", width = 175)\n",
    "\n",
    "\n",
    "# Define interactivity functions\n",
    "from time import sleep\n",
    "def my_widget_input_handler(attr, old, new):\n",
    "    x1 = np.repeat(sX1.value, nsamples)\n",
    "    x2 = np.repeat(sX2.value, nsamples)\n",
    "    n = 1\n",
    "    pr = logistic(betas[0] + betas[1]*x1 + betas[2]*x2)\n",
    "    y =  np.random.binomial(n, pr)\n",
    "    \n",
    "\n",
    "    plot_data.data = {'x1': x1, 'x2': x2, 'y': y}\n",
    "    #dot2.glyph.fill_color = '#FF4500'\n",
    "    circle3.glyph.fill_alpha = 1\n",
    "    circle3.glyph.line_alpha = 1\n",
    "    circle3.glyph.size = 12\n",
    "    vd.glyph.line_alpha = 1\n",
    "    \n",
    "    vhist, vedges = np.histogram(y, density=True, bins=20) #vertical histogram\n",
    "    vzeros = np.zeros(len(vedges)-1)\n",
    "    vmax = max(vhist)*1.1\n",
    "    source_hist.data = {\"left\": np.repeat(0,len(vedges)-1), \"bottom\": vedges[:-1], \n",
    "                        \"top\": vedges[1:], \"right\": vhist/vhist.sum()}\n",
    "    \n",
    "    x = pd.Series(y).sort_values()\n",
    "    source_density.data = {'x': pdf(x, n, pr), \n",
    "                           'y': x}\n",
    "\n",
    "\n",
    "# Define widgets callbacks\n",
    "sX1.on_change('value', my_widget_input_handler)\n",
    "sX2.on_change('value', my_widget_input_handler)\n",
    "\n",
    "\n",
    "# Show the plot\n",
    "\n",
    "\n",
    "# Show the plot\n",
    "layout = row(p, pv, column(sX1,sX2))\n",
    "#layout = row(p)    \n",
    "handler2 = FunctionHandler(modify_doc)\n",
    "app2 = Application(handler2)\n",
    "show(app2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efectos marginales no lineales dependen del valor del predictor\n",
    "\n",
    "Una diferencia importante entre modelos lineales y no lineales es el cuidado que hay que tener cuando se interpretan los coeficientes. En un modelo lineal los $\\beta$ son efectos marginales/derivadas i.e. cuanto cambia `Y` con un cambio en `X` (i.e. la primera derivada es una constante). En un modelo no lineal es más complicado, la derivada depende de la función no lineal, y puede ocurrir que los efectos marginales/derivadas varian con el nivel de `X` (i.e. la primera derivada podría incluir `X`). En términos generales, con regresiones no lineales es mejor siempre interpretar los coeficientes **dado** un `X`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Veamos dos regresiones, una lineal y otra no-lineal, con la misma data \n",
    "# Primero veamos los datos\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "sn.boxplot(x=\"diagnosis\", y=\"radius_mean\", data=cancer, ax = ax) \n",
    "#sn.swarmplot(x=\"diagnosis\", y=\"radius_mean\", data=cancer, color = 'black', alpha=0.2, ax = ax)\n",
    "ax.set_ylabel('Tamaño del tumor (radio)')\n",
    "ax.set_xlabel('M: maligno, B: benigno')\n",
    "ax.set_title('Box plot')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parece que los tumores malignos son más grandes. No es sorpresivo pero confirmemoslo con una regresión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Regresión lineal OLS\n",
    "model = smf.ols('diagnosis_recode ~ radius_mean', data = cancer)\n",
    "results = model.fit()\n",
    "print(\"El efecto marginal es: \" + str(round(results.params[1],2))) #Tumores benignos tienen un radio más pequeño (p<0.05) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El tamaño del tumor afecta que se codifique como maligno o no, pero la pendiente y el intercepto son de una línea. Ese -0.1 no es cambio en la probabilidad de ser un tumor maligno; es una pendiente. \n",
    "\n",
    "Ahora tratemos una regresión logística dado que el diagnóstico es binario, maligno o benigno, y con esta regresión podemos obtener probabilidades.\n",
    "\n",
    "Nos interesa el cambio de probabilidad de ser maligno con un aumento en el tamaño. Recordar que en una regresión logística, la formula de la regresión es igual al logit:\n",
    "\n",
    "$$ \n",
    "\\begin{equation} \n",
    "x_j\\beta_j = log\\left(\\frac{p}{1-p}\\right) = log(p) - log(1-p)\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "La primera derivada, que nos daría el efecto marginal de interés, va igual depender de x pues p lo obtenemos de la función logística. Esta es la primera derivada (ver https://stats.stackexchange.com/questions/19764/marginal-effect-of-probit-and-logit-model):\n",
    "\n",
    "$$ efecto \\; marginal \\; de \\; j = \\beta_jp(1-p)$$\n",
    "\n",
    "Donde p es:\n",
    "\n",
    "$$ p = \\frac{1}{1+exp^{-x\\beta}}$$\n",
    "\n",
    "Por ello hay que decidir en cuál x se evalua el efecto marginal. Tal vez el más directo es en el promedio de `X`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Regresión logistica GLM \n",
    "model = smf.glm('diagnosis_recode ~ radius_mean', data = cancer, \n",
    "                family = sm.families.Binomial())\n",
    "results = model.fit()\n",
    "xBeta = results.params[0]+results.params[1]*cancer['radius_mean'].mean() # punto a evaluar\n",
    "p = 1/(1+np.exp(-xBeta))\n",
    "em = results.params[1]*p*(1-p) # efecto marginal\n",
    "print(\"Una reducción en tamaño disminuye la probabilidad de ser maligno en \" + str(round(em,2)) + \" (en el promedio).\")\n",
    "print(\"El tamaño afecta la probabilidad relativa de ser maligno relativo a benigno, la cual es \" + \n",
    "      str(round(1/np.exp(results.params[1]),2)) + \" (odds ratio)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diferentes sentidos de likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# data artificial \n",
    "nobs=100\n",
    "mean = 400\n",
    "stddev = 200\n",
    "data_dgp = np.random.normal(mean, stddev, nobs)\n",
    "fig, ax = plt.subplots(figsize = (5,5))\n",
    "sn.scatterplot(x = np.arange(len(data_dgp)), y = data_dgp, ax = ax)\n",
    "\n",
    "\n",
    "# likelihood es la probabilidad de los datos\n",
    "hipotesis_mean = 200 #asumamos que sabemos sd\n",
    "den = scipy.stats.norm.pdf(data_dgp, hipotesis_mean, stddev)\n",
    "l = np.prod(den) # likelihood de la data (underflow, se va a cero rápidamente)\n",
    "print('Este es el likelihood: ' + str(round(l,3)))\n",
    "ll = np.log(den).sum() #log likelihood de la data. En logaritmos, la multiplicación se vuelve suma y no hay underflow.\n",
    "print('Este es el log likelihood: ' + str(round(ll,3)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El likelihood es la probabilidad de la data dada la hipótesis. ¿Es una buena hipótesis? ¿buen log likelihood? Hay que probar más hipótesis. Usaremos el log likelihood para evitar underflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# No confundir likelihood con MLE que algunas veces se visualiza como sigue \n",
    "# Probemos varios promedios para ver cual es mejor \n",
    "hipotesis_mean = np.linspace(-275,1000, 100)\n",
    "ll = [] #log likelihoods\n",
    "for hypothesis in hipotesis_mean:\n",
    "    den = scipy.stats.norm.pdf(data_dgp, hypothesis, stddev)\n",
    "    ll.append(np.log(den).sum())\n",
    "ll = np.array(ll)\n",
    "fig, ax = plt.subplots(figsize = (5,5))\n",
    "sn.lineplot(x = hipotesis_mean, y = ll, ax = ax)\n",
    "ax.set_ylabel('log. likelihood')\n",
    "ax.set_xlabel('hipótesis')\n",
    "ax.set_title('MLE: p(parametros|data)')\n",
    "idx = ll == max(ll)\n",
    "ax.scatter(hipotesis_mean[idx], ll[idx], s = 100, c = 'red', label = 'MLE')\n",
    "ax.legend()\n",
    "\n",
    "# La hipótesis que maximiza el likelihood (MLE) es la que definimos antes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum likelihood estimation (MLE)\n",
    "\n",
    "El MLE es una alternativa a mínimos cuadrados ordinarios (OLS). Para obtener el estimativo MLE muchas veces (sino la mayoria) tenemos que usar métodos numéricos y estrategias computacionales que nos permiten buscar maximizar la probabilidad de los datos entre varias posibles hipótesis de parametros. En regresiones lineales simples habiamos usado OLS. Hagamoslo con MLE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear\n",
    "# Lo primero es definir una función que nos diga cómo calcular el (log) likelihood\n",
    "def ll(params):\n",
    "    # log likelihood (ll)\n",
    "    # Esta función es la que vamos a pasar a un algoritmo\n",
    "    # para que proponga parametros (params) que minimice\n",
    "    # el log. likelihood negativo\n",
    "    \n",
    "    intercept = params[0]\n",
    "    beta1 = params[1]\n",
    "    sigma = params[2]\n",
    "    \n",
    "    mu = intercept + beta1*x\n",
    "    l = scipy.stats.norm.pdf(y, mu, sigma) #likelihood\n",
    "    l[l==0] = 1**(-300) #Hack to avoid infinity (beware: only for pedagogical purposes)\n",
    "    ll = -np.log(l).sum() #negative log likelihood (el algoritmo va a minimizar)\n",
    "    return ll \n",
    "\n",
    "# Parametros para iniciar el algoritmo\n",
    "guess = np.array([0,0,0.1])\n",
    "\n",
    "# info global que usa la funcion likelihood\n",
    "x = cancer['radius_mean']\n",
    "y = cancer['diagnosis_recode'] \n",
    "\n",
    "optim = minimize(ll, guess, method = 'L-BFGS-B',\n",
    "                 bounds= ((-10, 10), (-10, 10), (0,10)), #a tuple for each parameter (as ordered in the likelihood function)\n",
    "                 options={'disp': True})\n",
    "results_mle = pd.DataFrame({'coef':optim['x']})\n",
    "results_mle.index=['Intercept','beta1','sigma']   \n",
    "model = smf.ols('diagnosis_recode~radius_mean', data = cancer)\n",
    "results_ols = model.fit()\n",
    "print('RESULTADOS MLE')\n",
    "print(np.round(results_mle.head(3), 4)) \n",
    "print('\\n')\n",
    "print('RESULTADOS OLS')\n",
    "print(results_ols.summary()) \n",
    "\n",
    "#Compare los resultados con MLE y OLS. Deben ser iguales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión no lineal: Poisson\n",
    "\n",
    "Otra regresion para modelar conteos es usando una distribución discreta como la Poisson\n",
    "\n",
    "$$ y \\sim Poisson(\\theta)$$\n",
    "\n",
    "Donde\n",
    "\n",
    "$$ \\theta = e^{x\\beta}$$\n",
    "\n",
    "Veamos un ejemplo con una data que tiene el conteo de cuantas veces paran a una persona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Poisson police stops ####\n",
    "# stops es cuantas veces la policia para a una persona para requisar\n",
    "# constant term\n",
    "# ¿Por que usamos para offset past.arrests? baseline\n",
    "model_null = smf.glm('stops ~ 1', offset=np.log(Police['past.arrests']), \n",
    "                family = sm.families.Poisson(), data = Police)\n",
    "results_null = model_null.fit(cov_type='HC3')\n",
    "print(results_null.summary())\n",
    "\n",
    "\n",
    "# ethnicity indicator eth 1=black, 2=hispanic, 3=white.\n",
    "model_A = smf.glm('stops ~ eth', offset = np.log(Police['past.arrests']), \n",
    "                family = sm.families.Poisson(), data = Police)\n",
    "results_A = model_A.fit(cov_type='HC3')\n",
    "print(results_A.summary())\n",
    "\n",
    "\n",
    "# ethnicity & precints indicators\n",
    "model_B = smf.glm('stops ~ eth + precinct', offset = np.log(Police['past.arrests']), \n",
    "                family = sm.families.Poisson(), data = Police)\n",
    "results_B = model_B.fit(cov_type='HC3')\n",
    "print(results_B.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "ejercicio"
    ]
   },
   "source": [
    "### Ejercicio de interpretación\n",
    "\n",
    "Modelo A:\n",
    "\n",
    "* Interpretar coeficientes exp(coef).  eth 1=black,\t2=hispanic,3=white (recordar: es relativo al baseline i.e. la categoria que no aparece eth:1:black). ¿A quíen arrestan más que los negros? ¿Hispanos o blancos?.\n",
    "* Comparar modelo A y modelo null. Use el AIC (método .aic de las variables `results`) (menor es mejor). ¿vale la pena meter un regresor de etnicidad?\n",
    "\n",
    "Modelo B:\n",
    "\n",
    "* Interpretar coeficientes de etnicidad. \n",
    "* ¿Por qué blancos ahora si es significativo?\n",
    "* ¿Por qué ahora es más negativo el coeficiente para blancos eth:3?\n",
    "* Comparar AIC con modelo A que solo tiene etnicidad (menor es mejor). ¿Vale la pena meter el precinto?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propensity Score Matching (PSM)\n",
    "\n",
    "Si usamos probit o logit (que es usual), el propensity score es la probabilidad de estar en un grupo (en python, si el fit está en results, entonces el score es results.predict). Si los grupos son tratamiento y control, queremos que ambos tengan scores similares i.e. que la probabilidad de ser tratado sea parecida, dado los covariates (e.g. demográficos). En otras palabras, que ambos grupos sean lo más parecidos posibles. Los algoritmos de PSM usan alguna regla para decidir qué es un score cercano (e.g. k nearest neighbor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# pscore-match documentation (http://www.kellieottoboni.com/pscore_match/index.html) \n",
    "idx = HISP['round']==0 #para hacer matching con características antes del tratamiento\n",
    "HISP_baseline = HISP.loc[idx].copy() \n",
    "#clasificar households en funcion de unas caracteristicas\n",
    "treatment = np.array(HISP_baseline['enrolled'])\n",
    "model = smf.glm('enrolled ~ age_hh + age_sp + educ_hh + educ_sp +'\\\n",
    "                'female_hh + indigenous + hhsize + dirtfloor + bathroom +'\\\n",
    "                'land + hospital_distance', \n",
    "                data = HISP_baseline, \n",
    "                family = sm.families.Binomial())\n",
    "results = model.fit()\n",
    "pscore = results.predict()\n",
    "pairs = Match(treatment, pscore)\n",
    "#many-to one: one control individual is matched to many treatment individuals\n",
    "#one-to one: one control individual is matched to one treatment individual\n",
    "##si no sirve, cambiar el codigo de whichMatched de data.ix a data.iloc\n",
    "pairs.create(method='many-to-one', many_method='knn', k=5, replace=True) \n",
    "#Otros metodos:\n",
    "# ‘one-to-one’ or ‘many-to-one’\n",
    "# many_method: (solo para many-to-one)\n",
    "#       \"caliper\" (default) to select all matches within a given range, \"knn\" for k nearest neighbors,  \n",
    "HISP_baseline.insert(0,'pscore',pscore) \n",
    "HISP_baseline.insert(0,'treatment',treatment) \n",
    "HISP_matched = whichMatched(pairs, HISP_baseline) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la base HISP_matched, los grupos tratamiento y control son lo más parecido posibles dado los demográficos que definimos en la lista de covariables. Veamos como se veian los propensity scores antes y después."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.subplot(121)\n",
    "density0 = gaussian_kde(pscore[treatment==0])\n",
    "density1 = gaussian_kde(pscore[treatment==1])\n",
    "xs = np.linspace(0,1,200)\n",
    "plt.plot(xs,density0(xs),color='black', label = 'control')\n",
    "plt.fill_between(xs,density1(xs),color='gray', label = 'tratamiento')\n",
    "plt.legend()\n",
    "#plt.text(0.03, 35, 'Control Group')\n",
    "#plt.text(0.06, 10, 'Treatment Group')\n",
    "plt.title('Before Matching')\n",
    "plt.axis([0,1,0,3.5])\n",
    "plt.xlabel('Propensity Score')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "plt.subplot(122)\n",
    "density0_post = gaussian_kde(HISP_matched.pscore[HISP_matched.treatment==0])\n",
    "density1_post = gaussian_kde(HISP_matched.pscore[HISP_matched.treatment==1])\n",
    "xs = np.linspace(0,1,200)\n",
    "plt.plot(xs,density0_post(xs),color='black')\n",
    "plt.fill_between(xs,density1_post(xs),color='gray')\n",
    "plt.title('After Matching')\n",
    "plt.axis([0,1,0,3.5])\n",
    "plt.xlabel('Propensity Score')\n",
    "plt.ylabel('Density')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note como después de hacer el PSM, la probabilidad de estar en el grupo de control o tratamiento es bien parecida: las distribuciones del propensity score se sobrelapan. \n",
    "\n",
    "Hicimos matching con la data en el momento 0, antes de hacer el tratamiento. Para hacer análisis como Diff-In-Diff, tenemos que traer la información del momento 1, después de implementar el tratamiento. En la base de datos HISP tenemos identificadores (household_identifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Este loop toma tiempo, 30-60 segundos\n",
    "for idx, hhi in enumerate(HISP_matched['household_identifier']):\n",
    "    idx2 = (HISP['household_identifier'] == hhi) & (HISP['round'] == 1)\n",
    "    infoTemp = HISP.loc[idx2,:].copy()\n",
    "    infoTemp.insert(0,'pscore', float('nan')) \n",
    "    infoTemp.insert(0,'treatment', float('nan')) \n",
    "    HISP_matched = pd.concat([HISP_matched, infoTemp])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos hacer nuestros análisis con la nueva base pareada por PSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Podemos hacer diferencia en diferencias con la nueva base de datos\n",
    "model = smf.ols(\"health_expenditures~round*enrolled + age_hh + age_sp +\"\\\n",
    "           \"educ_hh + educ_sp + female_hh + indigenous + hhsize +\"\\\n",
    "           \"dirtfloor + bathroom + land + hospital_distance\",\n",
    "         data = HISP_matched)\n",
    "results = model.fit()\n",
    "print(results.summary()) #La interaccion es el diff in diff \n",
    "print('\\n')\n",
    "\n",
    "idx0 = (HISP['enrolled'] == 0) & (HISP['round'] == 1)\n",
    "idx1 = (HISP['enrolled'] == 1) & (HISP['round'] == 1)\n",
    "eff = HISP.loc[idx0,'health_expenditures'].mean() - HISP.loc[idx1,'health_expenditures'].mean()\n",
    "print(\"Mayor gastos de bolsillo en salud en control: \" + str(round(eff,2)))\n",
    "\n",
    "\n",
    "idx0 = (HISP_matched['enrolled'] == 0) & (HISP_matched['round'] == 1)\n",
    "idx1 = (HISP_matched['enrolled'] == 1) & (HISP_matched['round'] == 1)\n",
    "eff = HISP_matched.loc[idx0,'health_expenditures'].mean() - HISP_matched.loc[idx1,'health_expenditures'].mean()\n",
    "print(\"Mayor gastos de bolsillo en salud en control (muestra PSM): \" + str(round(eff,2)))\n",
    "print('\\n')\n",
    "print(\"La política de enroll, haciendo match, parece menos fuerte, evitamos un sesgo.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio - Hacer propensity score matching (PSM)\n",
    "\n",
    "Vamos a analizar una base de datos que tiene información de pacientes vivos y muertos (http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/Crhc.html). Nos interesa saber el efecto de hacer un cateterismo al lado derecho del corazon. Haga lo siguiente:\n",
    "\n",
    "1. Cargue la base de datos `corazon.csv` (en la carpeta DataSets). Pongala en una variable llamada `corazon`.\n",
    "2. La variable `swang1` y `death` no son números. Recodifique la variable `swang1` para que RHC sea 1, y NO RHC sea 0. Haga lo mismo para la variable `death`.\n",
    "3. Haga una regresión logística con un generalized lineal model (Tip: use el método smf.glm). Obtenga el propensity score (Tip: use el método .predict del objeto .fit). \n",
    "    - Variable endógena: `death`\n",
    "    - Variable de interés: `swang1` (1: se hizo un cateterismo; 0: no se hizo)\n",
    "    - Variables de control: `age`, `sex`, `race`, `edu`, `income`, `ninsclas` (tipo de seguro),`renal` (problemas renales), `neuro` (problemas neurológicos).\n",
    "4. Haga una nueva base con grupos pareados por las variables de control del punto anterior. La variable de tratamiento es `death`. \n",
    "5. Grafique el propensity score antes y después del match (use el código donde hicimos esa gráfica para la base HISP). ¿Hay un buen match?\n",
    "6. Haga la misma regresión del punto 3 con la base pareada. ¿Hay diferencias?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo simple: modelemos la probabilidad de que salga un color en una bolsa de m&m.\n",
    "# Definamos una función para el likelihood\n",
    "\n",
    "def ll (params): \n",
    "    prBlue = params[0]\n",
    "    prBrown = params[1]\n",
    "    prGreen = params[2]\n",
    "    prOrange = params[3]\n",
    "    prRed = params[4]\n",
    "    prYellow = params[5]\n",
    "    probs = np.array([prBlue, prBrown, prGreen, prOrange, prRed, prYellow])\n",
    "    probs = probs/probs.sum()\n",
    "    R = []\n",
    "    for i in range(mandm.shape[0]):\n",
    "        pckg_content = np.array(mandm.iloc[i,1:mandm.shape[1]])\n",
    "        R.append(scipy.stats.multinomial.pmf(pckg_content, n=pckg_content.sum(), p=probs))\n",
    "        \n",
    "  \n",
    "    R[R==0] = 1**(-300) #Hack to avoid infinity (beware: only for pedagogical purposes)\n",
    "    ll = -np.log(R)\n",
    "    return ll.sum()\n",
    "\n",
    "guess = np.array([.5,.5,.5,.5,.5,.5]) #parametros iniciales para el algoritmo\n",
    "optim = minimize(ll, guess, method = 'L-BFGS-B',\n",
    "                 bounds= ((0.0001, 1), (0.0001, 1), (0.0001,1), (0.0001, 1), (0.0001, 1), (0.0001,1)), #a tuple for each parameter (as ordered in the likelihood function)\n",
    "                 options={'disp': True})\n",
    "results_mle = pd.DataFrame({'coef':optim['x']})\n",
    "results_mle.index=['prBlue','prBrown','prGreen','prOrange','prRed','prYellow'] \n",
    "#print(results_mle)\n",
    "\n",
    "\n",
    "est = results_mle['coef']/results_mle['coef'].sum() #estimated\n",
    "d = mandm.iloc[:,1:mandm.shape[1]].sum() #data\n",
    "est.index = d.index\n",
    "d = d/d.sum()\n",
    "plt.plot(d, label = 'Promedio data')\n",
    "plt.ylabel('Probability')\n",
    "plt.plot(est,'ro', label = 'MLE')\n",
    "plt.legend()\n",
    "\n",
    "#El MLE es el promedio de la data (no siempre es el caso para todas las distribuciones y datos; para este ejemplo si)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El ejemplo de m&m es interesante pero simple. No hay, por ejemplo, variables de control. Para hacer regresiones multinomiales podemos usar el paquete statsmodels. Empecemos con un problema multinomial donde las categorias no tienen un orden particular (e.g. los colores de m&m; o nombres de hospitales; o tipos de perros; o ...). \n",
    "\n",
    "Podemos pensar el problema multinomial sin orden como varias regresiones binomiales independientes. Escogemos una categoría pivote (K) y hacemos las regresiones relativo a esa categoria.\n",
    "\n",
    "$$ log\\left(\\frac{Pr(Y_i=1)}{Pr(Y_i=k)}\\right) = \\beta_1X_i \\\\\n",
    "log\\left(\\frac{Pr(Y_i=2)}{Pr(Y_i=k)}\\right) = \\beta_2X_i  \\\\ \n",
    "...  = ...\\\\\n",
    "log\\left(\\frac{Pr(Y_i=k-1)}{Pr(Y_i=k)}\\right) = \\beta_{k-1}X_i $$\n",
    "\n",
    "Si exponenciamos a ambos lados y despejamos:\n",
    "\n",
    "$$ Pr(Y_i=1) = Pr(Y_i=k)e^{\\beta_1X_i} \\\\\n",
    "Pr(Y_i=2) = Pr(Y_i=k)e^{\\beta_2X_i} \\\\\n",
    "...=... \\\\\n",
    "Pr(Y_i=3) = Pr(Y_i=k)e^{\\beta_{k-1}X_i} $$\n",
    "\n",
    "Dado que son independientes las regresiones, podemos obtener los parametros $\\beta$ con MLE de forma iterativa. Primero hacemos la regresión solo con la constante (categoria base). Luego en la siguiente iteración incluimos los parámetros uno a uno. El algoritmo propone parámetros en cada iteración hasta convergir (maximizar el likelihood).\n",
    "\n",
    "Veamos como se hace en Python con un par de métodos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fishing['mode_code'] = fishing['mode'].cat.codes #this changes categories to numbers (mnlogit asks for numbers in the categories, not names)\n",
    "fishing['mode'].unique() #charter = 0, private boat = 1, pier = 2, beach = 3\n",
    "model = smf.mnlogit('mode_code ~ price + crate', data = fishing) #mode of fishing as a function of price of mode and catch rate (crate)\n",
    "results = model.fit() \n",
    "print(results.summary()) # cada categoria se modela como una regresión binomial i.e. exp(coef) = odds ratio\n",
    "results.predict()\n",
    "model_margeff=results.get_margeff(at='overall', method='dydx', count=True)\n",
    "print(model_margeff.summary()) #effectos marginales\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
