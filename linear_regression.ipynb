{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresiones lineales\n",
    "\n",
    "En este modulo aprenderemos a hacer regresiones lineales para cortes transversales. Un corte transversal puede pensarse como una fotografía (datos) en un momento dado. Se enseñara, entonces, a analizar estas fotografías. Las técnicas que vamos a aprender incluyen variables instrumentales, diferencias en diferencias, y regresiones discontinuas. Todas comparten el objetivo de reducir sesgos en los estimativos y acercarse un poco más (ojo no del todo) a relaciones causales entre las variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminares\n",
    "\n",
    "Las regresiones pueden tener sesgos que evitan que podamos sobre-interpretar relaciones entre variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero importemos algunos paquetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "# para estructura de datos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# para gráficas e interacciones\n",
    "import matplotlib \n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sn\n",
    "import plotnine as p9\n",
    "from plotnine import ggplot, geom_point, geom_line, aes, geom_smooth, facet_wrap, themes\n",
    "import bqplot as bq\n",
    "from bqplot import pyplot as bqplt\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import GridspecLayout\n",
    "\n",
    "# para estadística \n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm \n",
    "#from statsmodels.sandbox.regression.gmm import IV2SLS \n",
    "from linearmodels.iv import IV2SLS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora cargemos algunas bases de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HISP: Health Insurance Subsidy Program\n",
    "HISP = pd.read_stata('DataSets/HISP/evaluation.dta')\n",
    "HISP_itr = pd.read_stata('DataSets/HISP/evaluation.dta', iterator=True) #tiene, entre otras cosas, la descripción de las variables\n",
    "#print(HISP.dtypes) #columnas con tipo de datos \n",
    "#print('\\n')\n",
    "#var_temp = 'female_hh' #cambiar para obtener descripción de la columna\n",
    "#print(\"La variable \" + var_temp + \" es: \" + HISP_itr.variable_labels()[var_temp])\n",
    "\n",
    "# Bangladesh data (Khandker et al 2010 worldbank book)\n",
    "bang = pd.read_csv(\"DataSets/Khandker2010/hh_98.csv\")\n",
    "bang[\"lexptot\"] = np.log(bang['exptot']) #log of expenditures\n",
    "bang[\"lnland\"] = np.log(1 + bang['hhland']/100) #log of land owned\n",
    "#print(bang.dtypes) #columnas con tipo de datos \n",
    "#print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heterocedasticidad\n",
    "\n",
    "Empecemos con un caso típico para entender por qué puede haber sesgos en las regresiones: heterocedasticidad. Pero primero veamos datos con homocedasticidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "X_data = np.linspace(1,100, n) \n",
    "Y_raw = 3.5 + 2.1 * X_data\n",
    "Y_noise = np.random.normal(loc = 0, scale = 5, size = n) #loc es el promedio, scale es desviación estandar\n",
    "Y = pd.DataFrame({\"X\": X_data, \"Y\": Y_raw + Y_noise})\n",
    "model = smf.ols(formula='Y ~ X', data = Y)\n",
    "results = model.fit()\n",
    "predictions = results.predict(Y['X'])\n",
    "residuals = results.resid\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (10,5)) #One figure (fig) and two subplots (ax1, ax2)\n",
    "ax1.plot(Y['X'], Y['Y'], 'ko')\n",
    "ax1.plot(Y['X'], predictions, 'r', linewidth=3)\n",
    "ax1.set(xlabel='X', ylabel='Y',\n",
    "       title='Homoscedasticidad \\n Ruido en medición parecido en todo x')\n",
    "sm.qqplot(residuals, line = 'r', ax = ax2)\n",
    "ax2.set(title='qqplot')\n",
    "plt.show()\n",
    "print(results.summary()) \n",
    "#Los coeficientes ¿Se parecen a los de Y_raw? \n",
    "#¡Si! Con homocedasticidad los estimativos lineales son bien cercanos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora veamos con heterocedasticidad. Esto es, la desviación estandar de Y depende del nivel de X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 500\n",
    "X_data = np.linspace(1,100, n)\n",
    "Y_raw = 1.37 + 2.097 * X_data\n",
    "def f(k,robust = False): #Esta función tiene comandos para la regresión y para gráficas.\n",
    "    f, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    #El ruido puede ser una función complicada que depende del nivel de la variable independiente X\n",
    "    Y_noise = np.random.normal(loc = 0, scale = X_data**k, size = n)\n",
    "    Y = pd.DataFrame({\"X\": X_data, \"Y\": Y_raw + Y_noise})\n",
    "    \n",
    "    # Regresión\n",
    "    model = smf.ols(formula='Y ~ X', data = Y)\n",
    "    if robust == True:\n",
    "        results = model.fit(cov_type='HC3') #errores standard robustos (a heterocedasticidad)\n",
    "    else:\n",
    "        results = model.fit() \n",
    "    residuals = results.resid\n",
    "    Y['predictions'] = results.predict(Y['X'])\n",
    "\n",
    "    #Gráficas\n",
    "    sn.scatterplot(x = 'X', y='Y', data=Y, color = 'black', ax = axes[0])\n",
    "    axes[0].set_title(\"Heterocedasticidad \\n Ruido en medición depende del nivel de x\")\n",
    "    axes[0].plot(Y['X'], Y['predictions'], color = 'red')\n",
    "    axes[1].axis('off')\n",
    "    const = str(round(results.params[0],3))\n",
    "    slope = str(round(results.params[1],3))\n",
    "    axes[1].text(0, 0.5, 'Verdad: Intercepto: 1.370; Coef. X: 2.097 \\nRegres: Intercepto: ' + const + '; Coef. X: ' + slope,\n",
    "        color='black', fontsize=15)\n",
    "    \n",
    "    #Output\n",
    "    plt.show()\n",
    "    print(results.summary()) \n",
    "    \n",
    "\n",
    "#Interactuar con gráficas\n",
    "interact(f, k = widgets.FloatSlider(min=0, max=3, step=.2, value=1.25), \n",
    "         robust = widgets.Checkbox(value = False, description = 'Std. err. robustos'));\n",
    "\n",
    "#Los coeficientes ¿Se parecen a los de Y_raw? \n",
    "#¡NO! Con heterocedasticidad, hay sesgo en los estimativos lineales, se pueden alejar de la realidad (incluso pueden ser significativos!)\n",
    "# Hay correcciones. Por ejemplo, active y desactive la regresion con errores estandar robustos (a heterocedasticidad; ver model.fit en función f). \n",
    "# Cada vez que activa o desactiva, el ruido cambia (ver función f), \n",
    "# pero en general se reduce nuestra incertidumbre e.g. ver std. err intercepto\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cómo sabemos si nuestra data tiene heterocedasticidad? Cualitativamente con plots QQ pero también hay tests.\n",
    "\n",
    "Usemos la data en HISP: Health Insurance Subsidy Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización\n",
    "f, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "sn.scatterplot(x = 'poverty_index', y='health_expenditures', \n",
    "               size = 'educ_hh', #el tamaño de los puntos sigue el valor de esta columna\n",
    "               data=HISP,\n",
    "              ax = axes[0])\n",
    "# La visualización parece mostrar con claridad la presencia de heterocedasticidad:\n",
    "# Los más pobres en general gastan poco en salud pero a medida que mejora el \n",
    "# indice de pobreza hay mucha variabilidad.\n",
    "\n",
    "# Regresión\n",
    "# Vamos a hacer algo que no es necesario en este ejemplo, pero que vale la pena aprender.\n",
    "# Dividimos la data en dos: train_data (para ajustar el modelo) y test_data (para predecir nuevos valores de y)\n",
    "msk = np.random.rand(HISP.shape[0]) < 0.5 #Boolean index (mask)\n",
    "train_data = HISP.loc[msk,:]\n",
    "test_data = HISP.loc[~msk,:] #esto es para predecir (ver abajo)\n",
    "if 'Intercept' not in test_data.columns:\n",
    "    test_data.insert(0,'Intercept',1) \n",
    "if 'poverty_index:educ_hh' not in test_data.columns:\n",
    "    test_data.insert(0,'poverty_index:educ_hh', test_data['poverty_index']*test_data['educ_hh'])\n",
    "# Escribimos la formula con *, para incluir todas las interacciones y efectos principales\n",
    "model = smf.ols(formula='health_expenditures ~ poverty_index*educ_hh', data = train_data) \n",
    "results = model.fit()\n",
    "npred = 100 #cantidad de valores a predecir\n",
    "predictions = results.predict(test_data.loc[:,['Intercept','poverty_index','educ_hh','poverty_index:educ_hh']])\n",
    "axes[0].scatter(test_data['poverty_index'], predictions, marker = 'o', color = 'red')\n",
    "\n",
    "# Inspección cualitativa\n",
    "residuals = results.resid\n",
    "sm.qqplot(residuals, line = 'r', ax = axes[1])\n",
    "axes[1].set(title='qqplot') # los residuales no son normales (los puntos azules no siguen la linea roja)\n",
    "\n",
    "# Tests de heterocedasticidad\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "from statsmodels.stats.diagnostic import het_white\n",
    "labels = ['LM-Statistic','LM-Test p-value', 'F-Statistic', 'F-Test p-value']\n",
    "white_test = het_white(residuals,  model.exog)\n",
    "bp_test = het_breuschpagan(residuals, model.exog)\n",
    "print(dict(zip(labels[2:4], bp_test[2:4]))) # p<0.05 hay heterocedasticidad\n",
    "print(dict(zip(labels[2:4], white_test[2:4]))) # p<0.05 hay heterocedasticidad\n",
    "\n",
    "#Incluyamos errores estandar robustos\n",
    "model = smf.ols(formula='health_expenditures ~ poverty_index*educ_hh', data = HISP) \n",
    "results = model.fit(cov_type='HC3')\n",
    "\n",
    "plt.show()\n",
    "print(results.summary()) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "#### Alternativa: regresión por percentiles\n",
    "\n",
    "Una alternativa, es analizar los datos por percentiles. Por ejemplo, en la gráfica anterior se podría analizar sólo los que más gastan en salud. Veamos como se haría."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regresión por percentiles\n",
    "model = smf.quantreg(formula='health_expenditures ~ poverty_index*educ_hh', data = train_data)\n",
    "def fq(q):\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    sn.scatterplot(x = 'poverty_index', y='health_expenditures', data=HISP, ax = ax)\n",
    "    results = model.fit(q=q)\n",
    "    predictions = results.predict(test_data.loc[:,['Intercept','poverty_index','educ_hh','poverty_index:educ_hh']])\n",
    "    ax.scatter(test_data['poverty_index'], predictions, marker = 'o', color = 'red')\n",
    "    plt.show()\n",
    "    print(results.summary())\n",
    "    \n",
    "\n",
    "# Visualización\n",
    "interact(fq, q = widgets.FloatSlider(min = 0.025, max = 0.975, step = .025, value = 0.5,\n",
    "                                    description = 'Percentile'))\n",
    "\n",
    "# Ponga el percentile en el menor valor con el slider. \n",
    "# Note como las predicciones de la regresión (linea roja) solo capturan el percentile más bajo de la variable dependiente (health_expenditure)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "Ejercicio"
    ]
   },
   "source": [
    "### Ejercicio 1 - Regresión\n",
    "\n",
    "Hemos aprendido que las regresiones pueden tener estimativos de coeficientes sesgados. En este ejercicio vamos a correr una regresión con heterocedasticidad. Haga lo siguiente:\n",
    "\n",
    "* Cargue la siguiente data con el siguiente comando: \n",
    "    * data = sm.datasets.engel.load_pandas().data\n",
    "\n",
    "* Inspeccione la data usando el método .head()\n",
    "\n",
    "* Haga una gráfica donde el eje x sea `income` y el eje y `expenditure`. Observando la gráfica, piensa que hay heterocedasticidad. ¿Por qué?\n",
    "\n",
    "* Haga una regresión tradicional donde la variable endógena sea `expenditure` y la exógena `income`. Con el comando print muestre los resultados del ajuste.\n",
    "\n",
    "* Haga un test de heterocedasticidad. ¿Qué le dice el p-valor?\n",
    "\n",
    "* Ahora ajuste el modelo con errores estandar robustos. ¿Qué cambio relativo a la regresión tradicional?\n",
    "\n",
    "* En la gráfica que hizo antes, ponga las predicciones de la regresión con una línea roja."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresiones y causalidad (cercanas a...)  \n",
    "\n",
    "¿Cómo sabemos si nuestro estimativo lineal está sesgado i.e. no refleja el proceso que generó la realidad? Con seguridad lo va a estar. Son modelos lineales para fenómenos complejos. Sin embargo, vamos a aprender técnicas que aumentan nuestra confianza de que las relaciones son más robustas que una simple correlación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Técnica 1: Diferencia en diferencias (DiD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a reproducir la tabla 7.3 Gertler et al 2016, WB book\n",
    "# Empecemos filtrando la data HISP Health Insurance Subsidy Program\n",
    "# Intervención a evaluar: subsidio para pagar seguro de salud (columna enrolled) \n",
    "# Variable a evaluar: gasto en salud del bolsillo (columna health_expenditures) \n",
    "# Por el momento, nos interesa localidades donde se intervino\n",
    "msk = HISP['treatment_locality'] == 1\n",
    "HISP_temp = HISP[msk].copy()\n",
    "HISP_temp.head()\n",
    "#print(HISP_temp.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de hacerlo en una regresión calculemos la diferencia en diferencias a mano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El punto en el tiempo antes y después esta en la columna round\n",
    "# La diferencia en diferencias sería:\n",
    "# (enrolled 0 round 0 - enrolled 0 round 1) -(enrolled 1 round 0 - enrolled 1 round 1) \n",
    "g = HISP_temp.groupby(['enrolled','round']).mean().reset_index() #¿Qué hace este comando?\n",
    "tratado = g['enrolled'] == 1\n",
    "no_tratado = g['enrolled'] == 0\n",
    "t1 = g['round'] == 1\n",
    "t0 = g['round'] == 0\n",
    "outcome = 'health_expenditures'\n",
    "efecto_tratados =  float(g.loc[(tratado) & (t0), outcome]) - float(g.loc[(tratado) & (t1), outcome])\n",
    "efecto_no_tratados =  float(g.loc[(no_tratado) & (t0), outcome]) - float(g.loc[(no_tratado) & (t1), outcome])\n",
    "DiD =  round(efecto_tratados -  efecto_no_tratados, 4)\n",
    "print(\"El efecto de la intervención fue una reducción en gastos de bolsillo en salud de \" + str(DiD) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora calculemos la DiD con una regresión. Lo que vamos a ver es que la diferencia en diferencias es el termino de interacción. En el contexto de la data HISP, el DiD nos dice cuanto subio (o bajo) el gasto en bolsillo en salud en un grupo vs. el otro en dos periodos de tiempo diferente i.e. diferencia en pendientes. Y eso precisamente es la interacción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DiD es el efecto interacción  \n",
    "# Note como escribimos la formula. Hubieramos podido usar *, pero la idea es aprender\n",
    "# que dos puntos (:) indica interacción. Esto es útil cuando hay más variables y no queremos \n",
    "# todas las interacciones (* hace todas y efectos solos).\n",
    "model = smf.ols(formula='health_expenditures ~ round:enrolled + round + enrolled', \n",
    "                data = HISP_temp)\n",
    "results = model.fit()\n",
    "print(results.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ¿Qué pasa si usamos errores robustos? \n",
    "# El coeficiente de la interacción (DiD) no cambia, solo los errores estandar\n",
    "model = smf.ols(formula = 'health_expenditures ~ round:enrolled + round + enrolled', \n",
    "                data = HISP_temp)\n",
    "results = model.fit(cov_type='HC3')\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ¿Qué pasa si introducimos controles demográficos?\n",
    "# Cambia la DD\n",
    "model = smf.ols(formula = 'health_expenditures ~ round:enrolled + round + enrolled +' \\\n",
    "                'age_hh + age_sp + educ_hh + educ_sp + indigenous +' \\\n",
    "                'female_hh + hhsize + dirtfloor + bathroom + land + hospital_distance', \n",
    "                data = HISP_temp)\n",
    "results = model.fit(cov_type='HC3')\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "Ejercicio"
    ]
   },
   "source": [
    "### Ejercicio 2 - Diferencia en Diferencias (DiD)\n",
    "\n",
    "Calcule el efecto del terrorismo en la economía del país Basco en España. Para ello usará la técnica de DiD, manualmente y con una regresión. Los datos están en la base `basque.csv`. De interés para este ejercicio, la base tiene:\n",
    "* Información en el periodo 1955 - 1997\n",
    "* Información de 18 regiones de España\n",
    "* El año de tratamiento es 1975. Tratamiento es aparición de terrorismo (ETA se formó en 1974).\n",
    "* La región tratada es 'Basque Country (Pais Vasco)'\n",
    "* Mediremos el impacto sobre la variable PIB per capita (`gdpcap`; está en miles)\n",
    "\n",
    "Su tarea es hacer lo siguiente:\n",
    "\n",
    "1. Cargar los datos a una variable con el nombre `basque` (basque.csv está en la carpeta DataSets).\n",
    "2. Crear una nueva base de datos con el nombre `basque_reducida`. Está nueva base solo tendrá las regiones `Basque Country (Pais Vasco)` y `Cataluna`. \n",
    "    * Tip 1: los nombres de las regiones están en la columna `regionname`. \n",
    "    * Tip 2: use indexación lógica (ver comienzo de esta sección) o el método `query()` de pandas dataframes.\n",
    "    * Tip 3: use el operador booleano o: `|`.\n",
    "3. Haga una gráfica con dos líneas, una para cada región. Eje x: `year`, Eje y: `gdpcap`. Describa qué pasa antes y después de 1975 en el País Basco relativo a Cataluña. \n",
    "    * Tip: grafique con la libreria seaborn (https://seaborn.pydata.org/generated/seaborn.lineplot.html). Recuerde que la importamos con el alias `sn`. Use el parametro 'hue'.\n",
    "4. Crear variables dummy en `basque_reducida` en dos columnas nuevas con los siguientes nombres:\n",
    "    - `post`: toma el valor 0 si la columna `year` es <1975, 1 de lo contrario\n",
    "    - `treat`: toma el valor 0 si la columna `regionname` es Cataluna, 1 de lo contrario\n",
    "\n",
    "5. Use el método `groupby` para calcular manualmente la DiD. Recuerde que nos interesa el PIB per capita (gdpcap). \n",
    "    * Tip: Use groupby con las columnas post y treat del punto anterior.\n",
    "6. Corra una regresión con las mismas columnas post y treat para obtener la DiD; no olvide incluir la interacción en la regresión. ¿Le dio igual que el cálculo manual?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Técnica 2: Regresiones con discontinuidades en el diseño (RDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empecemos reduciendo la data bang que cargamos al comienzo (ver razón abajo)\n",
    "# fuente: khandker et al 2010 worldbank book pg 212\n",
    "# dmmfd: Household has male microcredit participant: 1=Y, 0=N\n",
    "# dfmfd: Household has female microcredit participant: 1=Y, 0=N\n",
    "# hhland: Household amount of owned land\n",
    "# exptot: Household per capita total expenditure: Tk/year\n",
    "# Las ultimas dos (hhland y exptot) se analizaran en escala logaritmica (para respetar supuestos de normalidad)\n",
    "\n",
    "# The microcredit program was probabilistic/voluntary i.e. people above the land threshold\n",
    "# could or not participate (dmmfd o dfmfd). To produce a sharp \n",
    "# cutoff only those below the threshold who should be in the program \n",
    "# and those above who shouldn't are kept for the analysis.\n",
    "\n",
    "bang_d = bang.query('(hhland<50 & (dmmfd==1 | dfmfd==1)) |' \\\n",
    "                    '(hhland>=50 & (dmmfd==0 | dfmfd==0))')\n",
    "\n",
    "if 'tratamiento' not in bang_d.columns:\n",
    "    bang_d.insert(0, 'tratamiento', np.repeat(0, bang_d.shape[0]))\n",
    "idx = (bang_d['hhland']<50) & ((bang_d['dmmfd']==1) | (bang_d['dfmfd']==1)) \n",
    "bang_d.loc[idx,'tratamiento'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora hagamos la regresión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = np.log(1 + 50/100) #el threshold en escala logaritmica\n",
    "# lnland-cutoff: es el efecto de land normalizado a cero en el cutoff (facilita interpretación)\n",
    "# Usamos I() para hacer operaciones entre variables. Es la función identidad.\n",
    "model = smf.ols(formula='lexptot ~ I(lnland-cutoff)*tratamiento', \n",
    "                data = bang_d)\n",
    "results = model.fit(cov_type='HC3')\n",
    "print(results.summary()) \n",
    "# tratamiento no significativo no hay ATE (average treatment effect) en el cutoff (el \"salto\"); \n",
    "# interaccion casi significativa en el cutoff, puede haber cambio de pendiente pero no es seguro\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos en gráficas que de hecho no hay saltos en consumo evidentes antes y \n",
    "# después del umbral de eligibilidad para el microcredito\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "sn.scatterplot(x = 'lnland', y = 'lexptot', hue = 'tratamiento', \n",
    "               data = bang_d, ax = ax)\n",
    "if 'cutoff' not in band_d.columns:\n",
    "    bang_d.insert(0,'cutoff',np.repeat(cutoff,bang_d.shape[0]))\n",
    "\n",
    "# Izquierda del umbral\n",
    "dataT = bang_d.query('lnland<=cutoff')\n",
    "model = smf.ols(formula='lexptot ~ lnland', \n",
    "                data = dataT)\n",
    "results = model.fit(cov_type='HC3')\n",
    "predictions = results.predict(dataT['lnland'])\n",
    "if 'predictions' not in dataT.columns:\n",
    "    dataT.insert(0,'predictions',predictions)\n",
    "sn.lineplot(x = 'lnland', y = 'predictions', color = 'black',\n",
    "           data = dataT, ax = ax)\n",
    "\n",
    "# Derecha del umbral\n",
    "dataT = bang_d.query('lnland>cutoff')\n",
    "model = smf.ols(formula='lexptot ~ lnland', \n",
    "                data = dataT)\n",
    "results = model.fit(cov_type='HC3')\n",
    "predictions = results.predict(dataT['lnland'])\n",
    "if 'predictions' not in dataT.columns:\n",
    "    dataT.insert(0,'predictions',predictions)\n",
    "sn.lineplot(x = 'lnland', y = 'predictions', color = 'black',\n",
    "           data = dataT, ax = ax)\n",
    "\n",
    "ax.set_xlabel('Cantidad de tierra (log)')\n",
    "ax.set_ylabel('Consumo (log)')\n",
    "\n",
    "# En el punto que cambia de color los puntos, las lineas de regresión no saltan \n",
    "# No LATE: local average treatment effect. Vamos hablar más de esto luego. Basta decir que\n",
    "# el efecto causal (si hay) se daria en las proximidades del umbral (donde cambian de color los puntos)\n",
    "# La razón: justo antes y después del umbral se asume que los participantes son bien parecidos\n",
    "# Parece haber un cambio de pendiente pero ese no nos interesa por el momento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculemos un LATE (local average treatment effect) a mano.\n",
    "# Necesitamos definir el ancho del umbral (bandwidth). \n",
    "# En la figura anterior, equivale a definir el rango del eje x \n",
    "# donde queremos calcular el efecto en consumo antes y después \n",
    "# del cambio de color de los puntos\n",
    "\n",
    "bw = cutoff*0.5 #bandwidth NOTA: esto es una heuristica, hay otras que incluyen kernels triangulares, gaussianos, otros!\n",
    "if 'bw' not in bang_d.columns:\n",
    "    bang_d.insert(0,'bw',np.repeat(bw,bang_d.shape[0]))\n",
    "dataT = bang_d.query('lnland>cutoff & lnland<(cutoff+bw) & tratamiento == 0')\n",
    "R = dataT['lnland'].mean() #derecha del umbral\n",
    "dataT = bang_d.query('lnland<cutoff & lnland>(cutoff-bw) & tratamiento == 1')\n",
    "L = dataT['lnland'].mean() #izquierda del umbral\n",
    "print(\"LATE en consumo (escala log.): \" + str(round(R-L,2))) #escala eje y de la figura anterior\n",
    "print(\"LATE en consumo (escala lineal): \" + str(round(np.exp(R-L),2))) #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "Ejercicio"
    ]
   },
   "source": [
    "### Ejercicio 3 - Regresión discontinua (RDD)\n",
    "\n",
    "No se puede manejar si se consumió alcohol. Para aplicar este principio, las autoridades miden el nivel de alcohol en la sangre (NAS de ahora en adelante). Si el NAS supera un umbral, la persona es sancionada. Con una regresión discontinua, Hansen (2015) encontró evidencia causal que esta medida es efectiva para reducir la reincidencia de manejar bajo los efectos del alcohol. Con una data simulada parecida a la de Hansen, vamos a hacer parte de su análisis (haga todas las gráficas con seaborn o matplotlib, escoja solo uno).\n",
    "\n",
    "Haga lo siguiente:\n",
    "\n",
    "1. Cargue la data `BAC.csv` (en la carpeta DataSets) en una variable llamada `BAC` . La data tiene medidas promedios de varios conductores reincidentes para distintos NAS (la unidad de análisis no es conductor, es NAS). \n",
    "2. Imprima el nombre de las columnas con el método .dtypes\n",
    "3. Chequee si las variables sociodemográficas `Genero_promedio`,`Accidente_en_escena_promedio`,`Edad_promedio_estandarizada`, `Raza_blanco_promedio`, cambian en el valor del `Umbral_NAS`. Es decir, haga para cada variable una gráfica: Eje x: `Nivel_Alcohol_Sangre_NAS`. Eje y: variable. Visualmente revise si hay saltos en el umbral. Bono: ponga todas las graficas en una sola figura con plt.subplots().\n",
    "3. Cree una nueva columna llamada `Distancia_a_umbral`. Para crearla, reste las columnas `Nivel_Alcohol_Sangre_NAS` y `Umbral_NAS`.\n",
    "4. Corra dos regresiones lineales y guarde los resultados en distintas variables. La primera con datos que tengan `Nivel_Alcohol_Sangre_NAS` menor o igual al `Umbral_NAS`. La segunda con datos que tengan `Nivel_Alcohol_Sangre_NAS` mayor al `Umbral_NAS`\n",
    "5. Haga la siguiente gráfica:\n",
    "    - Eje x: `Nivel_Alcohol_Sangre_NAS`. Eje y = `Reincidencia_promedio`\n",
    "    - Añada dos lineas rojas con las predicciones de las regresiones que hizo en el punto 3.\n",
    "6. Corra una regresión lineal con las siguientes características:\n",
    "    - Variable dependiente: `Reincidencia_promedio`\n",
    "    - Variables independientes: `Incumplio_umbral`, `Distancia_a_umbral`, y la interacción\n",
    "    - Errores estandar robustos\n",
    "7. Interprete los resultados. ¿Cuál parámetro nos indica la discontinuidad?\n",
    "8. Introduzca controles sociodemográficos en la regresión (las demás columnas).\n",
    "9. Calcule el LATE alrededor del salto. Use el procedimiento que aprendimos antes con un bandwith de 0.02. \n",
    "10. Interprete el LATE.\n",
    "\n",
    "\n",
    "Referencias:\n",
    "\n",
    "Hansen, B. (2015). Punishment and deterrence: Evidence from drunk driving. American Economic Review, 105(4), 1581-1617."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Técnica 3 - Variables instrumentales\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empezemos con un ejemplo con data simulada de la cual sabemos la formula verdadera. \n",
    "# Fuente: Health econometrics using stata; 1st edition 2017, pp = 202-\n",
    "\n",
    "np.random.seed(123456) #esto hace que los numeros aleatorios sean los mismos cada corrida\n",
    "nobs = 10000\n",
    "x = np.random.normal(size = nobs) # exogenous variable \n",
    "w = np.random.normal(size = nobs) # instrumental variable\n",
    "u = np.random.normal(size = nobs) # omitted variable (unobserved)\n",
    "e1 = np.random.normal(size = nobs) # outcome error\n",
    "e2 = np.random.normal(size = nobs) # endogenous error\n",
    "y2 = x + .2*w + u + e2 # endogenous equation. y2 es una variable de interés que afecta y1 (e.g. educación) \n",
    "y1 = y2 + x + u + e1 # outcome equation (e.g. y1 es colesterol). No la conocemos, por eso usamos regresiones.\n",
    "data_dict = {'x': x, 'w': w, 'u': u, 'y2': y2, 'y1': y1}\n",
    "data_sim = pd.DataFrame(data_dict)\n",
    "\n",
    "# La regresión con u da un buen estimativo de como y2 afecta y1\n",
    "# i.e. sabemos que es 1 por el outcome equation que definimos\n",
    "model = smf.ols(\"y1 ~ y2 + x + u\", data = data_sim) #Esta es la regresión que hariamos si tuvieramos TODAS las variables\n",
    "results = model.fit(cov_type='HC3')\n",
    "print(results.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La regresión sin u (variables no observadas) da un estimativo sesgado del coeficiente de y2. \n",
    "# Razón: las variables no observadas (u) y la variable independiente y2\n",
    "# se correlacionan positivamente (ver endogenous equation). \n",
    "# Es decir, y2 carga información de variables no observadas. Al no incluir u, \n",
    "# la regresión asigna a y2 otra relevancia de lo que verdad tiene. \n",
    "\n",
    "# Por ejemplo, si y1 es colesterol, y2 es educación, y u es salud mental,\n",
    "# entonces al quitar salud mental de la ecuación, y asumamos que salud mental se\n",
    "# correlaciona con niveles de educación obtenidos, el efecto de educación\n",
    "# se estima mal pues no incluye la variable no observada que afecta tanto a\n",
    "# y1 (colesterol) y y2 (educación).\n",
    "\n",
    "model = smf.ols(\"y1 ~ y2 + x\", data = data_sim)\n",
    "results = model.fit(cov_type='HC3')\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La pregunta natural es si u son variables no observadas ¿No estamos atascados entonces? Una solución es usar variables instrumentales (IV, por sus siglas en inglés). Note en la formula de y2 que hay una variable w. Esta variable tiene propiedades interesantes:\n",
    "\n",
    "* Se correlaciona con y2. \n",
    "* No se correlaciona  con u (no observadas).\n",
    "* Afecta a y1 via y2.\n",
    "\n",
    "Como vimos en clase, esas características son claves para pensar a w como una variable instrumental. La idea básica es que vamos a reemplazar y2 usando w. Ese reemplazo se denomina instrumentalizar a y2 con w. Una forma de hacerlo es con 2SLS.  Veamos como implementamos esta idea para reducir el sesgo de no incluir variables no observadas en la regresión. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables instrumentales via 2SLS (Two-stage least squares)\n",
    "etapa1 = smf.ols('y2 ~ x + w', data = data_sim) #stage 1: \n",
    "results = etapa1.fit(cov_type='HC3')\n",
    "#print(results.summary())\n",
    "y2_hat = results.predict(data_sim.loc[:,['x','w']])\n",
    "if 'y2_hat' not in data_sim.columns:\n",
    "    data_sim.insert(0, 'y2_hat', y2_hat)\n",
    "etapa2 = smf.ols('y1 ~ x + y2_hat', data = data_sim) #stage 2:\n",
    "results = etapa2.fit(cov_type='HC3')\n",
    "print(results.summary())\n",
    "\n",
    "# Note como el coeficiente de y2_hat y x son más cercanos a lo definido en outcome equation.\n",
    "# A pesar de no observar variables criticas (u), y2_hat ya incluia esa información (i.e. u va al ruido de la etapa1).\n",
    "# El poder de este procedimiento es que soluciona el problema de variables omitidas.\n",
    "# El reto es encontrar buenos instrumentos w (ver anterior markdown para qué es un buen instrumento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Es recomendable usar funciones para hacer de forma directa la 2SLS. \n",
    "# Estas funciones están optimizadas, evita errores, tienen métodos adicionales,  \n",
    "# y vuelve fácil leer su código para usos futuros. \n",
    "if 'Intercept' not in data_sim.columns:\n",
    "    data_sim.insert(0, 'Intercept', np.repeat(1 ,data_sim.shape[0]))\n",
    "resultsIV = IV2SLS(dependent = data_sim['y1'],\n",
    "                   exog = data_sim[['Intercept','x']],\n",
    "                   endog = data_sim['y2'],\n",
    "                   instruments = data_sim['w']).fit()\n",
    "print(resultsIV.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ¿Como saber si necesitamos instrumentos y son buenos? Primer filtro: sentido común. Pero también hay tests.\n",
    "# Algunos tests importantes de exogeneidad, bondad de instrumentos, y especificación.\n",
    "\n",
    "# Empecemos añadiendo un instrumento nuevo\n",
    "if 'w2' not in data_sim.columns: #otro instrumento\n",
    "    data_sim.insert(0, 'w2', data_sim['w']**2)\n",
    "instruments = ['w', 'w2']\n",
    "resultsIV = IV2SLS(dependent = data_sim['y1'],\n",
    "                   exog = data_sim[['Intercept','x']],\n",
    "                   endog = data_sim['y2'],\n",
    "                   instruments = data_sim[instruments]).fit()\n",
    "\n",
    "print(resultsIV.summary)\n",
    "print('\\n')\n",
    "print(resultsIV.wu_hausman()) #p<0.05 hay endogeneidad en el OLS i.e. incluir variable instrumental ayuda\n",
    "print('\\n')\n",
    "print(resultsIV.wooldridge_regression) #p<0.05 hay endogeneidad en el OLS i.e. incluir variable instrumental ayuda\n",
    "print('\\n')\n",
    "print(resultsIV.sargan) #p<0.05 significa que uno o varios de los instrumentos no se necesitan (sobreidentificacion)\n",
    "print('\\n')\n",
    "print(data_sim[['y2'] + instruments].corr()) #correlacion entre endogenas e instrumentos tiene que ser alta para ser buen instrumento \n",
    "\n",
    "# Los tests nos dicen que necesitamos instrumentos.\n",
    "# Sin embargo, el instrumento w2 no es importante, la correlación es baja con la endogena y2.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 4 - Variables instrumentales\n",
    "\n",
    "La data que vamos a usar en este ejercicio tiene gastos en salud del bolsillo y varias características individuales. Haga una regresión con variables instrumentales.\n",
    "\n",
    "1. Cargue los datos `mus06data.dta` en una variable llamada mi_data. El archivo está en la carpeta DataSets. Recuerde, para cargar datos con la terminación .dta debe usar pd.read_stata().\n",
    "2. Obtenga la descripción de las columnas. Para ello cree otra variable llamada mi_data_descripcion. Use de nuevo pd.read_stata() pero ahora ponga la opción iterator = True. Luego escriba en una nueva linea el siguiente comando: mi_data_descripcion.variable_labels(). Lea y entienda la descripción de las siguientes variables: \n",
    "    - `ldrugexp ` (variable dependiente)\n",
    "    - `totchr age female blhisp linc` (variables exógenas)\n",
    "    - `hi_empunion` (variable endógena a instrumentalizar)\n",
    "    - `ssiratio, lowincome, firmsz, multlc` (instrumentos)\n",
    "3. Antes de calcular cualquier estadistica, sustente por qué `ssiratio` podría ser un buen instrumento para `hi_empunion`. SSI es Supplemental Security Income. Es un programa que paga beneficios a adultos y niños que tienen ingresos y recursos limitados (https://www.ssa.gov/ssi/).\n",
    "4. Calcule la correlación entre los instrumentos y la variable endógena. ¿Cuál parece ser el mejor y peor instrumento?\n",
    "5. Haga una regresión lineal simple con las variables descritas en el punto 2, SIN los instrumentos. Imprima la tabla de resultados. Observe el coeficiente de `hi_empunion`.\n",
    "6. Ahora haga una regresión lineal con la variable instrumental `ssiratio`. Use la función IV2SLS. En caso que no la haya importado, este es el comando: from linearmodels.iv import IV2SLS.\n",
    "7. Comente la diferencia entre los coeficientes para `hi_empunion` de la regresión hecha para el punto 5 y 6. ¿Por qué difieren?\n",
    "8. Haga una regresion con todas las variables instrumentales `ssiratio, lowincome, firmsz, multlc`. Para esta regresión corra los tests de wu_hausman(), wooldridge_regression, y de sargan. ¿Qué significan?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
